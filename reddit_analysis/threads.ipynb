{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Reddit Data from Ethereum, Bitcoin, and Ethereum Classic Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below analysis was in examination of the public-facing information contained in the Ethereum, Bitcoin, and Ethereum Classic subreddits. However, the code can be manipulated to analyze information in any subreddit.\n",
    "#### It does need a lot of cleaning up which I will do once I've drafted my results and analysis, so stay tuned for changes and any suggestions are greatly appreciated. I will eventually turn this into an analysis library for the LIbreQDA Project. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV file output from Scrapy into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open CSV files from Scrapy output\n",
    "# your schema from the Scrapy output should be: author; tagline; time_rel; comments; title; time_all(this is where dow, mon, day, time, year, tz, and dtg come from)\n",
    "# your final schema should be: author; tagline; time_rel(relative time of post from date of viewing); comments(' comments' removed); title; dow(day of the week 3-letter); mon(month 3-letter); day(1-31); time(24 hour clock); year; tz(timezone); dtg(date/time group).\n",
    "\n",
    "# To parse 'comments' to int\n",
    "# threads_eth['comments'] = threads_eth['comments'].map(lambda x: x.rstrip(' comments'))\n",
    "# parser for the 'time_all' will be posted when I have it functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import CSV with final schema into DataFrames\n",
    "\n",
    "threads_eth = pd.read_csv('redditData/threads_eth_fix.csv')\n",
    "threads_btc = pd.read_csv('redditData/threads_btc_fix.csv')\n",
    "threads_etc = pd.read_csv('redditData/threads_etc_fix.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysis of Reddit Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Graph of author posts for Ethereum\n",
    "\n",
    "eth_author = threads_eth.author\n",
    "eth_author_bar = eth_author.value_counts().head(25).plot(kind='bar')\n",
    "eth_author_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Graph of author posts for Bitcoin\n",
    "\n",
    "btc_author = threads_btc.author\n",
    "btc_author_bar = btc_author.value_counts().head(25).plot(kind='bar')\n",
    "btc_author_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Graph of author posts for Ethereum Classic\n",
    "\n",
    "etc_author = threads_etc.author\n",
    "etc_author_bar = etc_author.value_counts().head(25).plot(kind='bar')\n",
    "etc_author_bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments per author in ETH, BTC, or ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comments from Ethereum with authors\n",
    "eth_comments = threads_eth[['author', 'comments']].copy()\n",
    "\n",
    "# Highest number of comments on unique post by author\n",
    "eth_comments.sort_values(by='comments', ascending=False).head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total number of comments received by author for all author's posts\n",
    "\n",
    "eth_comment_tot = eth_comments.groupby('author').sum()\n",
    "eth_comment_tot_sort = eth_comment_tot.sort_values(by='comments', ascending=False).head(30).plot(kind='bar')\n",
    "eth_comment_tot_sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comments from Bitcoin with authors\n",
    "btc_comments = threads_btc[['author', 'comments']].copy()\n",
    "\n",
    "# Highest number of comments on unique post by author\n",
    "btc_comments.sort_values(by='comments', ascending=False).head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total number of comments received by author for all author's posts\n",
    "\n",
    "btc_comment_tot = btc_comments.groupby('author').sum()\n",
    "btc_comment_tot_sort = btc_comment_tot.sort_values(by='comments', ascending=False).head(30).plot(kind='bar')\n",
    "btc_comment_tot_sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comments from Ethereum Classic with authors\n",
    "etc_comments = threads_etc[['author', 'comments']].copy()\n",
    "\n",
    "# Highest number of comments on unique post by author\n",
    "etc_comments.sort_values(by='comments', ascending=False).head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total number of comments received by author for all author's posts\n",
    "\n",
    "etc_comment_tot = etc_comments.groupby('author').sum()\n",
    "etc_comment_tot_sort = etc_comment_tot.sort_values(by='comments', ascending=False).head(30).plot(kind='bar')\n",
    "etc_comment_tot_sort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of authors that are found in ETH, BTC, and ETC forums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the above Series into DataFrames\n",
    "\n",
    "df_eth = pd.Series.to_frame(eth_author.drop_duplicates(keep='first'))\n",
    "df_btc = pd.Series.to_frame(btc_author.drop_duplicates(keep='first'))\n",
    "df_etc = pd.Series.to_frame(etc_author.drop_duplicates(keep='first'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count of unique authors in forums\n",
    "\n",
    "unique_author = pd.DataFrame({'ETH': [len(df_eth)], 'BTC': [len(df_btc)], 'ETC': [len(df_etc)]}).plot(kind='bar')\n",
    "unique_author\n",
    "print('BTC:', len(df_btc), 'ETC:', len(df_etc), 'ETH:',len(df_eth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with all authors\n",
    "\n",
    "df_authors = pd.DataFrame({}, columns=('btc', 'eth', 'etc'))\n",
    "df_authors.btc = df_btc.author\n",
    "df_authors.eth = df_eth.author\n",
    "df_authors.etc = df_etc.author\n",
    "# df_authors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count of which posts in one forum have been created by unique authors from another forum\n",
    "# Keep in mind that this is from the top 1000 posts; an author may have a top post in one but not another\n",
    "\n",
    "btc_in_eth = Counter(df_authors.btc.dropna().isin(eth_author) == True)\n",
    "btc_in_etc = Counter(df_authors.btc.dropna().isin(etc_author) == True)\n",
    "eth_in_etc = Counter(df_authors.eth.dropna().isin(etc_author) == True)\n",
    "eth_in_btc = Counter(df_authors.eth.dropna().isin(btc_author) == True)\n",
    "etc_in_btc = Counter(df_authors.etc.dropna().isin(btc_author) == True)\n",
    "etc_in_eth = Counter(df_authors.etc.dropna().isin(eth_author) == True)\n",
    "\n",
    "multiple_author = pd.DataFrame({'BTC in ETH': [btc_in_eth[True]], 'BTC in ETC': [btc_in_etc[True]], 'ETH in BTC': [eth_in_btc[True]], 'ETH in ETC': [eth_in_etc[True]], 'ETC in BTC': [etc_in_btc[True]], 'ETC in ETH': [etc_in_eth[True]]})\n",
    "ma_graph = multiple_author.plot(kind='bar')\n",
    "ma_graph.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Table for above graph\n",
    "\n",
    "multiple_author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of author names that are found between ETH, BTC, and ETC forums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Author names from Bitcoin subreddit who posted in Ethereum subreddit\n",
    "\n",
    "authors_btc_in_eth = df_authors[df_authors.btc.isin(df_authors.eth)]\n",
    "authors_btc_in_eth = authors_btc_in_eth.btc\n",
    "authors_btc_in_eth = authors_btc_in_eth.dropna()\n",
    "\n",
    "# Author names from Ethereum subreddit who posted in Bitcoin subreddit\n",
    "\n",
    "authors_eth_in_btc = df_authors[df_authors.eth.isin(df_authors.btc)]\n",
    "authors_eth_in_btc = authors_eth_in_btc.eth\n",
    "authors_eth_in_btc = authors_eth_in_btc.dropna()\n",
    "\n",
    "# Author names from Bitcoin subreddit who posted in Ethereum Classic subreddit\n",
    "\n",
    "authors_btc_in_etc = df_authors[df_authors.btc.isin(df_authors.etc)]\n",
    "authors_btc_in_etc = authors_btc_in_etc.btc\n",
    "authors_btc_in_etc = authors_btc_in_etc.dropna()\n",
    "\n",
    "# Author names from Ethereum Classic subreddit who posted in Bitcoin subreddit\n",
    "\n",
    "authors_etc_in_btc = df_authors[df_authors.etc.isin(df_authors.btc)]\n",
    "authors_etc_in_btc = authors_etc_in_btc.etc\n",
    "authors_etc_in_btc = authors_etc_in_btc.dropna()\n",
    "\n",
    "# Author names from Ethereum subreddit who posted in Ethereum Classic subreddit\n",
    "\n",
    "authors_eth_in_etc = df_authors[df_authors.eth.isin(df_authors.etc)]\n",
    "authors_eth_in_etc = authors_eth_in_etc.eth\n",
    "authors_eth_in_etc = authors_eth_in_etc.dropna()\n",
    "\n",
    "# Author names from Ethereum Classic subreddit who posted in Ethereum subreddit\n",
    "\n",
    "authors_etc_in_eth = df_authors[df_authors.etc.isin(df_authors.eth)]\n",
    "authors_etc_in_eth = authors_etc_in_eth.etc\n",
    "authors_etc_in_eth = authors_etc_in_eth.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# authors_btc_in_eth\n",
    "# authors_eth_in_btc\n",
    "# authors_btc_in_etc\n",
    "# authors_etc_in_btc\n",
    "# authors_eth_in_etc\n",
    "# authors_etc_in_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a simple count of author's number of posts from the perspectives of the above results\n",
    "\n",
    "all_authors_names = list(authors_btc_in_etc) + list(authors_btc_in_eth) + list(authors_etc_in_btc) + list(authors_etc_in_eth) + list(authors_eth_in_btc) + list(authors_eth_in_etc)\n",
    "all_authors_names_count = Counter(all_authors_names)\n",
    "all_authors_names_count = pd.DataFrame.from_dict(all_authors_names_count, orient='index')\n",
    "# all_authors_names_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Table of authors found in one subreddit who have published in another subreddit\n",
    "\n",
    "all_authors_names_pivot = pd.DataFrame({}, columns=('names', 'btc_in_eth', 'eth_in_btc', 'btc_in_etc', 'etc_in_btc', 'eth_in_etc', 'etc_in_eth'))\n",
    "all_authors_names_pivot.names = all_authors_names_count.index.unique()\n",
    "all_authors_names_pivot = all_authors_names_pivot.fillna(value=0)\n",
    "all_authors_names_pivot2 = all_authors_names_pivot.set_index(['names'])\n",
    "\n",
    "# all_authors_names_pivot\n",
    "# all_authors_names_pivot2\n",
    "\n",
    "# Fill pivot2 with '1' for each instancy a unique author from one subreddit was found in another\n",
    "\n",
    "for x in list(all_authors_names_pivot.names):\n",
    "    if (x in list(authors_btc_in_etc)) == True:\n",
    "        all_authors_names_pivot2.ix[x, ['btc_in_etc']] += 1\n",
    "\n",
    "for x in list(all_authors_names_pivot.names):\n",
    "    if (x in list(authors_btc_in_eth)) == True:\n",
    "        all_authors_names_pivot2.ix[x, ['btc_in_eth']] += 1\n",
    "\n",
    "for x in list(all_authors_names_pivot.names):\n",
    "    if (x in list(authors_eth_in_btc)) == True:\n",
    "        all_authors_names_pivot2.ix[x, ['eth_in_btc']] += 1\n",
    "\n",
    "for x in list(all_authors_names_pivot.names):\n",
    "    if (x in list(authors_eth_in_etc)) == True:\n",
    "        all_authors_names_pivot2.ix[x, ['eth_in_etc']] += 1\n",
    "\n",
    "for x in list(all_authors_names_pivot.names):\n",
    "    if (x in list(authors_etc_in_btc)) == True:\n",
    "        all_authors_names_pivot2.ix[x, ['etc_in_btc']] += 1\n",
    "\n",
    "for x in list(all_authors_names_pivot.names):\n",
    "    if (x in list(authors_etc_in_eth)) == True:\n",
    "        all_authors_names_pivot2.ix[x, ['etc_in_eth']] += 1\n",
    "\n",
    "# Style the '1' values green within DataFrame output\n",
    "\n",
    "all_authors_names_pivot2.style.highlight_max(color='green')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series analysis of forums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Posts within General Reddit Timeframes - Date of Information (DOI): 26 December 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input for this funtion is DataFrame with column 'time_rel' from the Reddit scrape\n",
    "\n",
    "def posttime_rel(threads):\n",
    "    rel_time = threads.time_rel.value_counts()\n",
    "    rel_time.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posttime_rel(threads_eth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "posttime_rel(threads_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "posttime_rel(threads_etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts per month over all records (as of DOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input for this funtion is DataFrame with column 'mon' from the Reddit scrape\n",
    "\n",
    "def time_month_all(threads):\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    month_dict = dict(threads.mon.value_counts())\n",
    "    month_count = pd.DataFrame.from_dict(data=month_dict, orient='index')\n",
    "    month_count = month_count.rename(columns={'': 'mon', 0: 'count'})\n",
    "    month_count = month_count.reindex(months)\n",
    "    month_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_month_all(threads_eth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_month_all(threads_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_month_all(threads_etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts per year over all records (as of DOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input for this funtion is DataFrame with column 'year' from the Reddit scrape\n",
    "\n",
    "def time_year_all(threads):\n",
    "    years = [2013, 2014, 2015, 2016]\n",
    "    year_dict = dict(threads.year.value_counts())\n",
    "    year_count = pd.DataFrame.from_dict(data=year_dict, orient='index')\n",
    "    year_count = year_count.rename(columns={'': 'year', 0: 'count'})\n",
    "    year_count = year_count.reindex(years)\n",
    "    year_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_year_all(threads_eth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_year_all(threads_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_year_all(threads_etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post per day of the week over all records (as of DOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input for this funtion is DataFrame with column 'dow' from the Reddit scrape\n",
    "\n",
    "def time_dow_all(threads):\n",
    "    dows = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    dow_dict = dict(threads.dow.value_counts())\n",
    "    dow_count = pd.DataFrame.from_dict(data=dow_dict, orient='index')\n",
    "    dow_count = dow_count.rename(columns={'': 'dow', 0: 'count'})\n",
    "    dow_count = dow_count.reindex(dows)\n",
    "    dow_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_dow_all(threads_eth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_dow_all(threads_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_dow_all(threads_etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts per day of the month over all records (as of DOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input for this funtion is DataFrame with column 'day' from the Reddit scrape\n",
    "\n",
    "def time_day_all(threads):\n",
    "    days = list(range(1,32,1))\n",
    "    day_dict = dict(threads.day.value_counts())\n",
    "    day_count = pd.DataFrame.from_dict(data=day_dict, orient='index')\n",
    "    day_count = day_count.rename(columns={'': 'day', 0: 'count'})\n",
    "    day_count = day_count.reindex(days)\n",
    "    day_count.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_day_all(threads_eth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_day_all(threads_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_day_all(threads_etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Title Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds for all posts in ETH, BTC, or ETC subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create text files for wordclouds\n",
    "\n",
    "# regex for punctuation\n",
    "exclude = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "# write titles to file with punctuation replaced by spaces\n",
    "with open('redditData/threads_eth.txt', 'w') as eth_f:\n",
    "    for line in threads_eth['title']:\n",
    "        line_nopunct = exclude.sub(' ', line)\n",
    "        eth_f.write(line_nopunct + ' ')\n",
    "\n",
    "with open('redditData/threads_btc.txt', 'w') as btc_f:\n",
    "    for line in threads_btc['title']:\n",
    "        line_nopunct = exclude.sub(' ', line)\n",
    "        btc_f.write(line_nopunct + ' ')\n",
    "\n",
    "with open('redditData/threads_etc.txt', 'w') as etc_f:\n",
    "    for line in threads_etc['title']:\n",
    "        line_nopunct = exclude.sub(' ', line)\n",
    "        etc_f.write(line_nopunct + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wordcloud generator function for raw files\n",
    "\n",
    "def wcg(file):\n",
    "    text = open(file).read()\n",
    "    text = text.lower()\n",
    "    #reddit_mask = np.array(Image.open('sil.jpg'))\n",
    "    wc = WordCloud(width=1440, height=900, background_color='white') # and mask=reddit_mask for your desired JPG\n",
    "    wc.generate(text)\n",
    "    image_file = '%s.jpg' % file\n",
    "    wc.to_file(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JPG wordcloud files for all words in subreddit\n",
    "\n",
    "wcg('redditData/threads_eth.txt')\n",
    "wcg('redditData/threads_btc.txt')\n",
    "wcg('redditData/threads_etc.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to view output\n",
    "[ETH](redditData/threads_eth.txt.jpg)\n",
    "[BTC](redditData/threads_btc.txt.jpg)\n",
    "[ETC](redditData/threads_etc.txt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word frequency tables for ETH, BTC, and ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stop word list\n",
    "\n",
    "stopwords = ['-', '&', ',', ':', ';', '.', ',', 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'did', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'either', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fill', 'find', 'for', 'former', 'formerly', 'found', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on','once', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word frequency table for Ethereum\n",
    "\n",
    "with open('redditData/threads_eth.txt', 'r') as eth_f:\n",
    "    words_eth = eth_f.read()\n",
    "    wordlist_eth = words_eth.lower().split()\n",
    "    wordcount_eth = Counter(wordlist_eth)\n",
    "    wordcount_eth2 = pd.DataFrame.from_dict(wordcount_eth, orient='index').reset_index()\n",
    "    wordcount_eth2 = wordcount_eth2.rename(columns={'index':'word', 0:'count'})\n",
    "    \n",
    "# DataFrame of unique values not in stopword list in descending order\n",
    "wordcount_eth2[wordcount_eth2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False).head(30)\n",
    "\n",
    "# Summary of above eth2 DataFrame\n",
    "summary_wordcount_eth2 = wordcount_eth2[wordcount_eth2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False)\n",
    "summary_wordcount_eth2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word frequency table for Bitcoin\n",
    "\n",
    "with open('redditData/threads_btc.txt', 'r') as btc_f:\n",
    "    words_btc = btc_f.read()\n",
    "    wordlist_btc = words_btc.lower().split()\n",
    "    wordcount_btc = Counter(wordlist_btc)\n",
    "    wordcount_btc2 = pd.DataFrame.from_dict(wordcount_btc, orient='index').reset_index()\n",
    "    wordcount_btc2 = wordcount_btc2.rename(columns={'index':'word', 0:'count'})\n",
    "    \n",
    "# DataFrame of unique values not in stopword list in descending order\n",
    "wordcount_btc2[wordcount_btc2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False).head(30)\n",
    "\n",
    "# Summary of above btc2 DataFrame\n",
    "summary_wordcount_btc2 = wordcount_btc2[wordcount_btc2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False)\n",
    "summary_wordcount_btc2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word frequency table for Ethereum Classic\n",
    "\n",
    "with open('redditData/threads_etc.txt', 'r') as etc_f:\n",
    "    words_etc = etc_f.read()\n",
    "    wordlist_etc = words_etc.lower().split()\n",
    "    wordcount_etc = Counter(wordlist_etc)\n",
    "    wordcount_etc2 = pd.DataFrame.from_dict(wordcount_etc, orient='index').reset_index()\n",
    "    wordcount_etc2 = wordcount_etc2.rename(columns={'index':'word', 0:'count'})\n",
    "    \n",
    "# DataFrame of unique values not in stopword list in descending order\n",
    "wordcount_etc2[wordcount_etc2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False).head(30)\n",
    "\n",
    "# Summary of above etc2 DataFrame\n",
    "summary_wordcount_etc2 = wordcount_etc2[wordcount_etc2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False)\n",
    "summary_wordcount_etc2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word clouds for all posts in ETH, BTC, or ETC with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create stopword functions for ETH, BTC, and ETC from word frequency lists\n",
    "\n",
    "def stopword_eth(top_terms_remove):\n",
    "    stopword_eth = wordcount_eth2[wordcount_eth2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False).head(top_terms_remove)\n",
    "    stopword_eth = list(stopword_eth.word)\n",
    "    return stopword_eth\n",
    "\n",
    "# stopword_eth(5)\n",
    "\n",
    "def stopword_btc(top_terms_remove):\n",
    "    stopword_btc = wordcount_btc2[wordcount_btc2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False).head(top_terms_remove)\n",
    "    stopword_btc = list(stopword_btc.word)\n",
    "    return stopword_btc\n",
    "\n",
    "# stopword_btc(5)\n",
    "\n",
    "def stopword_etc(top_terms_remove):\n",
    "    stopword_etc = wordcount_etc2[wordcount_etc2['word'].map(lambda x: x not in stopwords)].sort_values('count', ascending=False).head(top_terms_remove)\n",
    "    stopword_etc = list(stopword_etc.word)\n",
    "    return stopword_etc\n",
    "\n",
    "# stopword_etc(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordcloud generator function for raw files with stopwords\n",
    "\n",
    "def wcg_stop(file, stop, number_top_terms):\n",
    "    text = open(file).read()\n",
    "    text = text.lower()\n",
    "    if stop == 'eth':\n",
    "        stop_wc = stopword_eth(number_top_terms)\n",
    "    elif stop == 'btc':\n",
    "        stop_wc = stopword_btc(number_top_terms)\n",
    "    elif stop == 'etc':\n",
    "        stop_wc = stopword_etc(number_top_terms)\n",
    "    wc = WordCloud(width=1440, height=900, background_color='white', stopwords=stop_wc + stopwords)\n",
    "    wc.generate(text)\n",
    "    wc.to_file('%s.jpg' % (file+('_notop_')+str(number_top_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wcg_stop('redditData/threads_eth.txt', 'eth', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcg_stop('redditData/threads_btc.txt', 'btc', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcg_stop('redditData/threads_etc.txt', 'etc', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word clouds per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create text files for wordclouds\n",
    "\n",
    "# regex for punctuation\n",
    "exclude = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "# write titles for specific Ethereum subreddit author to txt file\n",
    "# wordcloud of all Ethereum author posts\n",
    "def auth_wordcloud_eth(auth):\n",
    "    with open('redditData/threads_eth_auth.txt', 'w') as eth_fauth:\n",
    "        lines = threads_eth.loc[threads_eth['author'] == auth]\n",
    "        for line in lines['title']:\n",
    "            line_nopunct = exclude.sub(' ', line)\n",
    "            eth_fauth.write(line_nopunct + ' ')\n",
    "    auth_rename = 'redditData/threads_eth_%s.txt' % auth\n",
    "    os.rename('redditData/threads_eth_auth.txt', auth_rename)\n",
    "    wcg(auth_rename)\n",
    "\n",
    "# write titles for specific Bitcoin subreddit author to txt file\n",
    "# wordcloud of all Bitcoin author posts\n",
    "def auth_wordcloud_btc(auth):\n",
    "    with open('redditData/threads_btc_auth.txt', 'w') as btc_fauth:\n",
    "        lines = threads_btc.loc[threads_btc['author'] == auth]\n",
    "        for line in lines['title']:\n",
    "            line_nopunct = exclude.sub(' ', line)\n",
    "            btc_fauth.write(line_nopunct + ' ')\n",
    "    auth_rename = 'redditData/threads_btc_%s.txt' % auth\n",
    "    os.rename('redditData/threads_btc_auth.txt', auth_rename)\n",
    "    wcg(auth_rename)\n",
    "\n",
    "# write titles for specific Ethereum Classic subreddit author to txt file\n",
    "# wordcloud of all Ethereum Classic author posts\n",
    "def auth_wordcloud_etc(auth):\n",
    "    with open('redditData/threads_etc_auth.txt', 'w') as etc_fauth:\n",
    "        lines = threads_etc.loc[threads_etc['author'] == auth]\n",
    "        for line in lines['title']:\n",
    "            line_nopunct = exclude.sub(' ', line)\n",
    "            etc_fauth.write(line_nopunct + ' ')\n",
    "    auth_rename = 'redditData/threads_etc_%s.txt' % auth\n",
    "    os.rename('redditData/threads_etc_auth.txt', auth_rename)\n",
    "    wcg(auth_rename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# auth_wordcloud_eth('vbuterin')\n",
    "\n",
    "# generate Ethereum author word cloud of all author posts with stop words\n",
    "# wcg_stop('redditData/threads_eth_vbuterin.txt', 'eth', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# auth_wordcloud_btc('Egon_1')\n",
    "\n",
    "# generate Bitcoin author word cloud of all author posts with stop words\n",
    "# wcg_stop('redditData/threads_btc_Egon_1.txt', 'eth', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# auth_wordcloud_etc('bit_novosti')\n",
    "\n",
    "# generate Ethereum Classic author word cloud of all author posts with stop words\n",
    "# wcg_stop('redditData/threads_etc_bit_novosti.txt', 'eth', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word clouds per time frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Working on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Reddit Content to Interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
